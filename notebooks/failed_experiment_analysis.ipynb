{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triage all errors caught during experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the results file here. this file should be created by the script process_inbox.py\n",
    "results_csv = \"/Users/duncan/research/active_projects/reczilla/results/results.csv\"\n",
    "\n",
    "# define a function that takes a row as input an returns True if the row should be included in the meta-dataset and false otherwise\n",
    "def include_row(row):\n",
    "    if row[\"experiment_name\"].startswith(\"neurips-\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duncan/miniconda3/envs/recsys/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (13,14,16,17,29,30,35,37,38,39,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(results_csv, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the results based on the function \"include_row\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### keep experiments indicated by function include_row\n",
    "experiment_prefix = \"full-experiment-\"\n",
    "keep_rows = df.apply(include_row, axis=1)\n",
    "df_expt = df.loc[keep_rows, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze all failed jobs and exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### count number of results for each dataset-alg pair\n",
    "ignore_datasets = [\"AmazonBooksReader\", \"AmazonPurchaseCirclesReader\", \"GoogleLocalReviewsReader\"]\n",
    "df_tmp = df_expt.loc[~df_expt[\"dataset_name\"].isin(ignore_datasets), :].copy()\n",
    "knn_rows = df_tmp[\"alg_name\"].str.contains(\"KNN\")\n",
    "\n",
    "knn_basename = df_tmp.loc[knn_rows, \"alg_name\"].apply(lambda x: x.split(\"_\")[0])\n",
    "knn_sim = df_tmp.loc[knn_rows, \"alg_name\"].apply(lambda x: x.split(\"_\")[1])\n",
    "df_tmp.loc[knn_rows, \"alg_name\"] = knn_basename  # either UserKNN or ItemKNN\n",
    "\n",
    "# number of samples for each dataset-alg pair\n",
    "num_samples = df_tmp.groupby([\"alg_name\", \"dataset_name\"]).size().rename(\"num_samples\").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a complete list of all algs and datasets\n",
    "all_datasets = list(df_tmp[\"dataset_name\"].unique())\n",
    "all_algs = list(df_tmp[\"alg_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In num_samples, make sure there is one entry for each alg-dataset pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every combination of alg-dataset\n",
    "import itertools\n",
    "alg_dataset_pairs = [x for x in itertools.product(all_algs, all_datasets)]\n",
    "alg_list_tmp = [x[0] for x in alg_dataset_pairs]\n",
    "dataset_list_tmp = [x[1] for x in alg_dataset_pairs]\n",
    "\n",
    "num_samples = num_samples.merge(pd.DataFrame(\n",
    "    {\n",
    "        \"alg_name\": alg_list_tmp,\n",
    "        \"dataset_name\": dataset_list_tmp,\n",
    "        }), how=\"outer\")\n",
    "\n",
    "# set all new rows (with no samples) to zero\n",
    "num_samples.loc[num_samples[\"num_samples\"].isna(), \"num_samples\"] = 0\n",
    "\n",
    "# list of algs that only have one sample (no hyperparams)\n",
    "one_sample_algs = [\n",
    "    \"SlopeOne\",\n",
    "    \"TopPop\",\n",
    "    \"GlobalEffects\", \n",
    "    \"Random\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterized algs with fewer than 100 samples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alg_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>num_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>SLIMElasticNetRecommender</td>\n",
       "      <td>MarketBiasAmazonReader</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>P3alphaRecommender</td>\n",
       "      <td>AmazonKitchenDiningReader</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>AmazonDigitalMusicReader</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            alg_name               dataset_name  num_samples\n",
       "1527       SLIMElasticNetRecommender     MarketBiasAmazonReader          0.0\n",
       "1528              P3alphaRecommender  AmazonKitchenDiningReader          0.0\n",
       "1529  MatrixFactorization_BPR_Cython   AmazonDigitalMusicReader          0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples.loc[(num_samples[\"num_samples\"] < 100) & ~num_samples[\"alg_name\"].isin(one_sample_algs), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only three instances of this, and they all yield zero samples.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alg_name</th>\n",
       "      <th>alg_seed</th>\n",
       "      <th>cutoff_list</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>exception</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>hyperparameters_source</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>original_split_path</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>...</th>\n",
       "      <th>test_metric_USERS_IN_GT_cut_5</th>\n",
       "      <th>test_metric_USERS_IN_GT_cut_50</th>\n",
       "      <th>test_metric_USERS_IN_GT_cut_6</th>\n",
       "      <th>test_metric_USERS_IN_GT_cut_7</th>\n",
       "      <th>test_metric_USERS_IN_GT_cut_8</th>\n",
       "      <th>test_metric_USERS_IN_GT_cut_9</th>\n",
       "      <th>time</th>\n",
       "      <th>time_on_test</th>\n",
       "      <th>time_on_train</th>\n",
       "      <th>time_on_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 381 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [alg_name, alg_seed, cutoff_list, dataset_name, exception, experiment_name, hyperparameters_source, num_samples, original_split_path, param_alpha, param_asymmetric_alpha, param_batch_size, param_beta, param_beta_loss, param_confidence_scaling, param_epsilon, param_feature_weighting, param_init_type, param_item_reg, param_l1_ratio, param_l2_norm, param_lambda_i, param_lambda_j, param_learning_rate, param_n_cltr_i, param_n_cltr_u, param_negative_interactions_quota, param_negative_reg, param_normalize, param_normalize_avg_row, param_normalize_similarity, param_num_factors, param_positive_reg, param_reg, param_seed, param_sgd_mode, param_shrink, param_similarity_from_distance_mode, param_solver, param_symmetric, param_topK, param_tversky_alpha, param_tversky_beta, param_use_bias, param_user_reg, sample_number, split_name, test_metric_ARHR_ALL_HITS_cut_1, test_metric_ARHR_ALL_HITS_cut_10, test_metric_ARHR_ALL_HITS_cut_15, test_metric_ARHR_ALL_HITS_cut_2, test_metric_ARHR_ALL_HITS_cut_20, test_metric_ARHR_ALL_HITS_cut_3, test_metric_ARHR_ALL_HITS_cut_30, test_metric_ARHR_ALL_HITS_cut_4, test_metric_ARHR_ALL_HITS_cut_40, test_metric_ARHR_ALL_HITS_cut_5, test_metric_ARHR_ALL_HITS_cut_50, test_metric_ARHR_ALL_HITS_cut_6, test_metric_ARHR_ALL_HITS_cut_7, test_metric_ARHR_ALL_HITS_cut_8, test_metric_ARHR_ALL_HITS_cut_9, test_metric_AVERAGE_POPULARITY_cut_1, test_metric_AVERAGE_POPULARITY_cut_10, test_metric_AVERAGE_POPULARITY_cut_15, test_metric_AVERAGE_POPULARITY_cut_2, test_metric_AVERAGE_POPULARITY_cut_20, test_metric_AVERAGE_POPULARITY_cut_3, test_metric_AVERAGE_POPULARITY_cut_30, test_metric_AVERAGE_POPULARITY_cut_4, test_metric_AVERAGE_POPULARITY_cut_40, test_metric_AVERAGE_POPULARITY_cut_5, test_metric_AVERAGE_POPULARITY_cut_50, test_metric_AVERAGE_POPULARITY_cut_6, test_metric_AVERAGE_POPULARITY_cut_7, test_metric_AVERAGE_POPULARITY_cut_8, test_metric_AVERAGE_POPULARITY_cut_9, test_metric_COVERAGE_ITEM_HIT_cut_1, test_metric_COVERAGE_ITEM_HIT_cut_10, test_metric_COVERAGE_ITEM_HIT_cut_15, test_metric_COVERAGE_ITEM_HIT_cut_2, test_metric_COVERAGE_ITEM_HIT_cut_20, test_metric_COVERAGE_ITEM_HIT_cut_3, test_metric_COVERAGE_ITEM_HIT_cut_30, test_metric_COVERAGE_ITEM_HIT_cut_4, test_metric_COVERAGE_ITEM_HIT_cut_40, test_metric_COVERAGE_ITEM_HIT_cut_5, test_metric_COVERAGE_ITEM_HIT_cut_50, test_metric_COVERAGE_ITEM_HIT_cut_6, test_metric_COVERAGE_ITEM_HIT_cut_7, test_metric_COVERAGE_ITEM_HIT_cut_8, test_metric_COVERAGE_ITEM_HIT_cut_9, test_metric_COVERAGE_ITEM_cut_1, test_metric_COVERAGE_ITEM_cut_10, test_metric_COVERAGE_ITEM_cut_15, test_metric_COVERAGE_ITEM_cut_2, test_metric_COVERAGE_ITEM_cut_20, test_metric_COVERAGE_ITEM_cut_3, test_metric_COVERAGE_ITEM_cut_30, test_metric_COVERAGE_ITEM_cut_4, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 381 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look into these... (they have )\n",
    "df[(df[\"alg_name\"] == \"P3alphaRecommender\") & (df[\"dataset_name\"] == \"AmazonKitchenDiningReader\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alg_name</th>\n",
       "      <th>alg_seed</th>\n",
       "      <th>cutoff_list</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>exception</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>hyperparameters_source</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>original_split_path</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>...</th>\n",
       "      <th>test_metric_USERS_IN_GT_cut_5</th>\n",
       "      <th>test_metric_USERS_IN_GT_cut_50</th>\n",
       "      <th>test_metric_USERS_IN_GT_cut_6</th>\n",
       "      <th>test_metric_USERS_IN_GT_cut_7</th>\n",
       "      <th>test_metric_USERS_IN_GT_cut_8</th>\n",
       "      <th>test_metric_USERS_IN_GT_cut_9</th>\n",
       "      <th>time</th>\n",
       "      <th>time_on_test</th>\n",
       "      <th>time_on_train</th>\n",
       "      <th>time_on_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 381 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [alg_name, alg_seed, cutoff_list, dataset_name, exception, experiment_name, hyperparameters_source, num_samples, original_split_path, param_alpha, param_asymmetric_alpha, param_batch_size, param_beta, param_beta_loss, param_confidence_scaling, param_epsilon, param_feature_weighting, param_init_type, param_item_reg, param_l1_ratio, param_l2_norm, param_lambda_i, param_lambda_j, param_learning_rate, param_n_cltr_i, param_n_cltr_u, param_negative_interactions_quota, param_negative_reg, param_normalize, param_normalize_avg_row, param_normalize_similarity, param_num_factors, param_positive_reg, param_reg, param_seed, param_sgd_mode, param_shrink, param_similarity_from_distance_mode, param_solver, param_symmetric, param_topK, param_tversky_alpha, param_tversky_beta, param_use_bias, param_user_reg, sample_number, split_name, test_metric_ARHR_ALL_HITS_cut_1, test_metric_ARHR_ALL_HITS_cut_10, test_metric_ARHR_ALL_HITS_cut_15, test_metric_ARHR_ALL_HITS_cut_2, test_metric_ARHR_ALL_HITS_cut_20, test_metric_ARHR_ALL_HITS_cut_3, test_metric_ARHR_ALL_HITS_cut_30, test_metric_ARHR_ALL_HITS_cut_4, test_metric_ARHR_ALL_HITS_cut_40, test_metric_ARHR_ALL_HITS_cut_5, test_metric_ARHR_ALL_HITS_cut_50, test_metric_ARHR_ALL_HITS_cut_6, test_metric_ARHR_ALL_HITS_cut_7, test_metric_ARHR_ALL_HITS_cut_8, test_metric_ARHR_ALL_HITS_cut_9, test_metric_AVERAGE_POPULARITY_cut_1, test_metric_AVERAGE_POPULARITY_cut_10, test_metric_AVERAGE_POPULARITY_cut_15, test_metric_AVERAGE_POPULARITY_cut_2, test_metric_AVERAGE_POPULARITY_cut_20, test_metric_AVERAGE_POPULARITY_cut_3, test_metric_AVERAGE_POPULARITY_cut_30, test_metric_AVERAGE_POPULARITY_cut_4, test_metric_AVERAGE_POPULARITY_cut_40, test_metric_AVERAGE_POPULARITY_cut_5, test_metric_AVERAGE_POPULARITY_cut_50, test_metric_AVERAGE_POPULARITY_cut_6, test_metric_AVERAGE_POPULARITY_cut_7, test_metric_AVERAGE_POPULARITY_cut_8, test_metric_AVERAGE_POPULARITY_cut_9, test_metric_COVERAGE_ITEM_HIT_cut_1, test_metric_COVERAGE_ITEM_HIT_cut_10, test_metric_COVERAGE_ITEM_HIT_cut_15, test_metric_COVERAGE_ITEM_HIT_cut_2, test_metric_COVERAGE_ITEM_HIT_cut_20, test_metric_COVERAGE_ITEM_HIT_cut_3, test_metric_COVERAGE_ITEM_HIT_cut_30, test_metric_COVERAGE_ITEM_HIT_cut_4, test_metric_COVERAGE_ITEM_HIT_cut_40, test_metric_COVERAGE_ITEM_HIT_cut_5, test_metric_COVERAGE_ITEM_HIT_cut_50, test_metric_COVERAGE_ITEM_HIT_cut_6, test_metric_COVERAGE_ITEM_HIT_cut_7, test_metric_COVERAGE_ITEM_HIT_cut_8, test_metric_COVERAGE_ITEM_HIT_cut_9, test_metric_COVERAGE_ITEM_cut_1, test_metric_COVERAGE_ITEM_cut_10, test_metric_COVERAGE_ITEM_cut_15, test_metric_COVERAGE_ITEM_cut_2, test_metric_COVERAGE_ITEM_cut_20, test_metric_COVERAGE_ITEM_cut_3, test_metric_COVERAGE_ITEM_cut_30, test_metric_COVERAGE_ITEM_cut_4, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 381 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df[\"alg_name\"] == \"MatrixFactorization_BPR_Cython\") & (df[\"dataset_name\"] == \"AmazonDigitalMusicReader\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look into why these experiments failed, we will need to look at the log files.\n",
    "\n",
    "- SLIMElasticNetRecommender\t+ MarketBiasAmazonReader: (log file: log_340_050922_145822.txt): this job failed due to SSH timeout, not a code issue.\n",
    "- P3alphaRecommender + AmazonKitchenDiningReader: (log file: log_305_050422_230905.txt):  also failed due to SSH timeout.\n",
    "- MatrixFactorization_BPR_Cython + AmazonDigitalMusicReader: (log file: log_203_050722_191431.txt): same error as the other two...\n",
    "\n",
    "here is an example of what this error looks like:\n",
    "```\n",
    "launching instance neurips-llo-a-305...\n",
    "Created [https://www.googleapis.com/compute/v1/projects/research-collab-naszilla/zones/us-central1-a/instances/neurips-llo-a-305].\n",
    "NAME               ZONE           MACHINE_TYPE  PREEMPTIBLE  INTERNAL_IP    EXTERNAL_IP    STATUS\n",
    "neurips-llo-a-305  us-central1-a  n1-highmem-2               10.128.15.206  35.238.53.180  RUNNING\n",
    "successfully created instance: neurips-llo-a-305\n",
    "ssh: connect to host 35.238.53.180 port 22: Connection timed out\n",
    "\n",
    "Recommendation: To check for possible causes of SSH connectivity issues and get\n",
    "recommendations, rerun the ssh command with the --troubleshoot option.\n",
    "\n",
    "gcloud compute ssh neurips-llo-a-305 --project=research-collab-naszilla --zone=us-central1-a --troubleshoot\n",
    "\n",
    "Or, to investigate an IAP tunneling issue:\n",
    "\n",
    "gcloud compute ssh neurips-llo-a-305 --project=research-collab-naszilla --zone=us-central1-a --troubleshoot --tunnel-through-iap\n",
    "\n",
    "ERROR: (gcloud.compute.ssh) [/usr/bin/ssh] exited with return code [255].\n",
    "failed to run experiment during attempt 2... (exit code: 255)\n",
    "trying again in 30 seconds...\n",
    "ssh: connect to host 35.238.53.180 port 22: Connection timed out\n",
    "\n",
    "Recommendation: To check for possible causes of SSH connectivity issues and get\n",
    "recommendations, rerun the ssh command with the --troubleshoot option.\n",
    "\n",
    "gcloud compute ssh neurips-llo-a-305 --project=research-collab-naszilla --zone=us-central1-a --troubleshoot\n",
    "\n",
    "Or, to investigate an IAP tunneling issue:\n",
    "\n",
    "gcloud compute ssh neurips-llo-a-305 --project=research-collab-naszilla --zone=us-central1-a --troubleshoot --tunnel-through-iap\n",
    "\n",
    "ERROR: (gcloud.compute.ssh) [/usr/bin/ssh] exited with return code [255].\n",
    "failed to run experiment during attempt 3... (exit code: 255)\n",
    "too many SSH attempts. giving up and deleting instance.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION**:\n",
    "- these three instances are one-off SSH errors, we could re-run them if we'd like, but I wouldn't bother. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now look at one-sample jobs that failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alg_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>num_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [alg_name, dataset_name, num_samples]\n",
       "Index: []"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples.loc[(num_samples[\"num_samples\"] < 1) & num_samples[\"alg_name\"].isin(one_sample_algs), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it appears that there are results for all one-sample algs! this is good. let's look at the exceptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of exceptions for one-sample algs: 4\n",
      "total number of exceptions: 32\n",
      "        alg_name                       dataset_name\n",
      "1515    SlopeOne     AmazonAmazonInstantVideoReader\n",
      "3330    SlopeOne         AmazonSportsOutdoorsReader\n",
      "6915    SlopeOne     AmazonGroceryGourmetFoodReader\n",
      "7266    SlopeOne             AmazonVideoGamesReader\n",
      "12103   SlopeOne   AmazonToolsHomeImprovementReader\n",
      "22917   SlopeOne             AmazonAutomotiveReader\n",
      "23733   SlopeOne  AmazonCellPhonesAccessoriesReader\n",
      "28479   SlopeOne        AmazonPatioLawnGardenReader\n",
      "29071   SlopeOne                      GowallaReader\n",
      "37021   SlopeOne                      RecipesReader\n",
      "37154   SlopeOne            AmazonPetSuppliesReader\n",
      "41954   SlopeOne                     EpinionsReader\n",
      "52218   SlopeOne            AmazonElectronicsReader\n",
      "65504   SlopeOne           AmazonDigitalMusicReader\n",
      "69125   SlopeOne              AmazonToysGamesReader\n",
      "72451   SlopeOne               AmazonSoftwareReader\n",
      "73590   SlopeOne                       DatingReader\n",
      "82105   SlopeOne   AmazonClothingShoesJewelryReader\n",
      "88650   SlopeOne            AmazonKindleStoreReader\n",
      "90570   SlopeOne   AmazonIndustrialScientificReader\n",
      "90892   SlopeOne                   AmazonBabyReader\n",
      "107440  SlopeOne                 BookCrossingReader\n",
      "107671  SlopeOne            AmazonHomeKitchenReader\n",
      "109986  SlopeOne     AmazonMusicalInstrumentsReader\n",
      "112229  SlopeOne         AmazonAppsforAndroidReader\n",
      "112784  SlopeOne               AmazonCDsVinylReader\n",
      "118804  SlopeOne     AmazonHealthPersonalCareReader\n",
      "118805  SlopeOne          AmazonAmazonFashionReader\n",
      "119217  SlopeOne       AmazonArtsCraftsSewingReader\n",
      "120508  SlopeOne         AmazonOfficeProductsReader\n",
      "124724  SlopeOne               AmazonMoviesTVReader\n",
      "130366  SlopeOne                 AmazonBeautyReader\n"
     ]
    }
   ],
   "source": [
    "one_sample_exceptions = df_tmp.loc[df_tmp[\"alg_name\"].isin(one_sample_algs), \"exception\"].unique()\n",
    "print(f\"number of exceptions for one-sample algs: {len(one_sample_exceptions)}\")\n",
    "print(f\"total number of exceptions: {sum(~df_tmp.loc[df_tmp['alg_name'].isin(one_sample_algs), 'exception'].isna())}\")\n",
    "\n",
    "print(df_tmp[df_tmp['alg_name'].isin(one_sample_algs) & ~df_tmp[\"exception\"].isna()][[\"alg_name\", \"dataset_name\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: only SlopeOne failed... let's see why.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traceback (most recent call last):\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 402, in _objective_function\\n    result_dict, result_string, recommender_instance, train_time, evaluation_time = self._evaluate_on_validation(current_fit_parameters_dict)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/RandomSearch.py\", line 50, in _evaluate_on_validation\\n    current_fit_parameters\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 299, in _evaluate_on_validation\\n    recommender_instance\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/Base/Evaluation/Evaluator.py\", line 253, in evaluateRecommender\\n    results_dict = self._run_evaluation_on_selected_users(recommender_object, self.users_to_evaluate)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/Base/Evaluation/Evaluator.py\", line 455, in _run_evaluation_on_selected_users\\n    return_scores = True\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/Base/BaseRecommender.py\", line 146, in recommend\\n    scores_batch = self._compute_item_score(user_id_array, items_to_compute=items_to_compute)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/SurpriseAlgorithms/Wrappers.py\", line 219, in _compute_item_score\\n    freq_mask = self.surprise_model.freq[item_id_array, :] > 0\\nMemoryError\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sample_exceptions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traceback (most recent call last):\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 402, in _objective_function\\n    result_dict, result_string, recommender_instance, train_time, evaluation_time = self._evaluate_on_validation(current_fit_parameters_dict)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/RandomSearch.py\", line 50, in _evaluate_on_validation\\n    current_fit_parameters\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 293, in _evaluate_on_validation\\n    recommender_instance, train_time = self._fit_model(current_fit_parameters)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 283, in _fit_model\\n    **current_fit_parameters)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/SurpriseAlgorithms/Wrappers.py\", line 94, in fit\\n    self.surprise_model.fit(self.trainset)\\n  File \"surprise/prediction_algorithms/slope_one.pyx\", line 57, in surprise.prediction_algorithms.slope_one.SlopeOne.fit\\nMemoryError\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sample_exceptions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traceback (most recent call last):\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 402, in _objective_function\\n    result_dict, result_string, recommender_instance, train_time, evaluation_time = self._evaluate_on_validation(current_fit_parameters_dict)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/RandomSearch.py\", line 50, in _evaluate_on_validation\\n    current_fit_parameters\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 299, in _evaluate_on_validation\\n    recommender_instance\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/Base/Evaluation/Evaluator.py\", line 253, in evaluateRecommender\\n    results_dict = self._run_evaluation_on_selected_users(recommender_object, self.users_to_evaluate)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/Base/Evaluation/Evaluator.py\", line 455, in _run_evaluation_on_selected_users\\n    return_scores = True\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/Base/BaseRecommender.py\", line 146, in recommend\\n    scores_batch = self._compute_item_score(user_id_array, items_to_compute=items_to_compute)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/SurpriseAlgorithms/Wrappers.py\", line 226, in _compute_item_score\\n    counts = (has_rating_mask @ freq_mask.T.astype(\\'double\\'))\\n  File \"/home/shared/miniconda3/envs/reczilla/lib/python3.6/site-packages/scipy/sparse/base.py\", line 562, in __matmul__\\n    return self.__mul__(other)\\n  File \"/home/shared/miniconda3/envs/reczilla/lib/python3.6/site-packages/scipy/sparse/base.py\", line 473, in __mul__\\n    return self._mul_multivector(other)\\n  File \"/home/shared/miniconda3/envs/reczilla/lib/python3.6/site-packages/scipy/sparse/compressed.py\", line 482, in _mul_multivector\\n    other.ravel(), result.ravel())\\nMemoryError\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sample_exceptions[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION**:\n",
    "- All of these SlopeOne errors are memory errors... There are 32 total. So we would need to re-run SlopeOne on, basically, all datasets with increased memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptions from Multiple-sample jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of exceptions for multi-sample algs: 63\n",
      "total number of exceptions: 6322\n",
      "total num exceptions:\n",
      "alg_name\n",
      "EASE_R_Recommender                    3299\n",
      "IALSRecommender                          2\n",
      "ItemKNNCF                               87\n",
      "MatrixFactorization_AsySVD_Cython        1\n",
      "MatrixFactorization_BPR_Cython           6\n",
      "MatrixFactorization_FunkSVD_Cython       2\n",
      "NMFRecommender                        2620\n",
      "P3alphaRecommender                      98\n",
      "PureSVDRecommender                      59\n",
      "RP3betaRecommender                     100\n",
      "SLIM_BPR_Cython                          4\n",
      "UserKNNCF                               44\n",
      "Name: num_except, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# first, count the total number of exceptions for each alg & dataset pair..\n",
    "multi_sample_exceptions = df_tmp.loc[~df_tmp[\"alg_name\"].isin(one_sample_algs), \"exception\"].unique()\n",
    "print(f\"number of exceptions for multi-sample algs: {len(multi_sample_exceptions)}\")\n",
    "print(f\"total number of exceptions: {sum(~df_tmp.loc[~df_tmp['alg_name'].isin(one_sample_algs), 'exception'].isna())}\")\n",
    "\n",
    "# print(df_tmp[~df_tmp['alg_name'].isin(one_sample_algs) & ~df_tmp[\"exception\"].isna()][[\"alg_name\", \"dataset_name\"]])\n",
    "num_exceptions = df_tmp[~df_tmp['alg_name'].isin(one_sample_algs) & ~df_tmp[\"exception\"].isna()].groupby([\"alg_name\", \"dataset_name\"]).size().rename(\"num_except\").reset_index()\n",
    "# print(df_tmp[~df_tmp['alg_name'].isin(one_sample_algs) & ~df_tmp[\"exception\"].isna()].groupby([\"alg_name\", \"dataset_name\"]).size())\n",
    "\n",
    "# print this out for each alg\n",
    "print(\"total num exceptions:\")\n",
    "print(num_exceptions.groupby(\"alg_name\")[\"num_except\"].sum())\n",
    "# for alg in num_exceptions[\"alg_name\"].unique():\n",
    "#     total_exceptions = num_exceptions[num_exceptions[\"alg_name\"] == alg][\"num_except\"].sum()\n",
    "#     print(f\"total exceptions for alg {alg}: \\n{total_exceptions}\")\n",
    "    # print(num_exceptions[num_exceptions[\"alg_name\"]? == alg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num exceptions by dataset:\n",
      "dataset_name\n",
      "Jester2Reader                        231\n",
      "AnimeReader                          229\n",
      "AmazonIndustrialScientificReader     149\n",
      "AmazonAmazonInstantVideoReader       142\n",
      "AmazonMusicalInstrumentsReader       127\n",
      "AmazonArtsCraftsSewingReader         111\n",
      "AmazonDigitalMusicReader             110\n",
      "AmazonToolsHomeImprovementReader     110\n",
      "AmazonVideoGamesReader               110\n",
      "RecipesReader                        109\n",
      "AmazonAppsforAndroidReader           108\n",
      "AmazonPatioLawnGardenReader          107\n",
      "MovieTweetingsReader                 107\n",
      "AmazonOfficeProductsReader           106\n",
      "AmazonPetSuppliesReader              106\n",
      "DatingReader                         106\n",
      "Movielens20MReader                   106\n",
      "AmazonBabyReader                     106\n",
      "AmazonBeautyReader                   106\n",
      "AmazonAutomotiveReader               105\n",
      "BookCrossingReader                   105\n",
      "GowallaReader                        104\n",
      "AmazonToysGamesReader                104\n",
      "AmazonKindleStoreReader              104\n",
      "AmazonElectronicsReader              104\n",
      "AmazonSportsOutdoorsReader           104\n",
      "AmazonCellPhonesAccessoriesReader    104\n",
      "AmazonHealthPersonalCareReader       104\n",
      "EpinionsReader                       104\n",
      "AmazonClothingShoesJewelryReader     103\n",
      "                                    ... \n",
      "AmazonWineReader                      55\n",
      "AmazonPopReader                       55\n",
      "AmazonSoftwareReader                  53\n",
      "AmazonAllElectronicsReader            51\n",
      "AmazonAllBeautyReader                 48\n",
      "AmazonGiftCardsReader                 47\n",
      "AmazonClassicalReader                 46\n",
      "MarketBiasAmazonReader                44\n",
      "AmazonInternationalReader             43\n",
      "AmazonBuyaKindleReader                42\n",
      "CiaoDVDReader                         41\n",
      "AmazonDanceElectronicReader           41\n",
      "AmazonBabyProductsReader              40\n",
      "AmazonCollectiblesFineArtReader       39\n",
      "AmazonAmazonFashionReader             37\n",
      "WikilensReader                        35\n",
      "AmazonMagazineSubscriptionsReader     31\n",
      "AmazonAppliancesReader                31\n",
      "AmazonLuxuryBeautyReader              30\n",
      "YahooMoviesReader                     29\n",
      "FilmTrustReader                       28\n",
      "LastFMReader                          28\n",
      "Movielens100KReader                   28\n",
      "FrappeReader                          28\n",
      "MarketBiasModClothReader              28\n",
      "AmazonChristianReader                 28\n",
      "MovielensHetrec2011Reader             27\n",
      "Movielens1MReader                     26\n",
      "Movielens10MReader                    10\n",
      "NetflixPrizeReader                     2\n",
      "Name: num_except, Length: 85, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# now group by dataset\n",
    "print(\"total num exceptions by dataset:\")\n",
    "print(num_exceptions.groupby(\"dataset_name\")[\"num_except\"].sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the exceptions come from EASE_R - let's see what they are.\n",
    "\n",
    "There is no especially problematic dataset -- though Jester and Anime have more than others. Probably because they are larger? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 exceptions for ease-r\n"
     ]
    }
   ],
   "source": [
    "# look at ease-r exceptions\n",
    "ease_r_excepts = df_tmp.loc[(df_tmp[\"alg_name\"] == \"EASE_R_Recommender\"), \"exception\"].unique()\n",
    "print(f\"{len(ease_r_excepts)} exceptions for ease-r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traceback (most recent call last):\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 402, in _objective_function\\n    result_dict, result_string, recommender_instance, train_time, evaluation_time = self._evaluate_on_validation(current_fit_parameters_dict)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/RandomSearch.py\", line 50, in _evaluate_on_validation\\n    current_fit_parameters\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 293, in _evaluate_on_validation\\n    recommender_instance, train_time = self._fit_model(current_fit_parameters)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 283, in _fit_model\\n    **current_fit_parameters)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/EASE_R/EASE_R_Recommender.py\", line 56, in fit\\n    grahm_matrix = similarity.compute_similarity().toarray()\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/Base/Similarity/Compute_Similarity.py\", line 126, in compute_similarity\\n    return self.compute_similarity_object.compute_similarity(**args)\\n  File \"Compute_Similarity_Cython.pyx\", line 439, in Compute_Similarity_Cython.Compute_Similarity_Cython.compute_similarity\\nMemoryError\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ease_r_excepts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traceback (most recent call last):\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 402, in _objective_function\\n    result_dict, result_string, recommender_instance, train_time, evaluation_time = self._evaluate_on_validation(current_fit_parameters_dict)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/RandomSearch.py\", line 50, in _evaluate_on_validation\\n    current_fit_parameters\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 293, in _evaluate_on_validation\\n    recommender_instance, train_time = self._fit_model(current_fit_parameters)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 283, in _fit_model\\n    **current_fit_parameters)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/EASE_R/EASE_R_Recommender.py\", line 65, in fit\\n    P = np.linalg.inv(grahm_matrix)\\n  File \"/home/shared/miniconda3/envs/reczilla/lib/python3.6/site-packages/numpy/linalg/linalg.py\", line 551, in inv\\n    ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj)\\nMemoryError\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ease_r_excepts[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all memory errors...\n",
    "\n",
    "**CONCLUSION**: \n",
    "- there were 3299 exceptions thrown for EASE-R - all memory errors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many of these exceptions are memory errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mem_excepts = sum(\n",
    "    ~df_tmp[\"alg_name\"].isin(one_sample_algs) \n",
    "    & (df_tmp[\"exception\"].str.contains(\"memory\") | df_tmp[\"exception\"].str.contains(\"Memory\"))\n",
    ")\n",
    "num_total_excepts = sum(\n",
    "    ~df_tmp[\"alg_name\"].isin(one_sample_algs) \n",
    "    & ~df_tmp[\"exception\"].isna()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 3402 memory exceptions out of 6322 total\n"
     ]
    }
   ],
   "source": [
    "print(f\"there are {num_mem_excepts} memory exceptions out of {num_total_excepts} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignoring NMF: there are 3383 memory exceptions out of 3702 total\n"
     ]
    }
   ],
   "source": [
    "## if we ignore NMF, how many memory errors?\n",
    "num_mem_excepts_2 = sum(\n",
    "    ~df_tmp[\"alg_name\"].isin(one_sample_algs) & (df_tmp[\"alg_name\"] != \"NMFRecommender\")\n",
    "    & (df_tmp[\"exception\"].str.contains(\"memory\") | df_tmp[\"exception\"].str.contains(\"Memory\"))\n",
    ")\n",
    "num_total_excepts_2 = sum(\n",
    "    ~df_tmp[\"alg_name\"].isin(one_sample_algs) & (df_tmp[\"alg_name\"] != \"NMFRecommender\")\n",
    "    & ~df_tmp[\"exception\"].isna()\n",
    ")\n",
    "print(f\"ignoring NMF: there are {num_mem_excepts_2} memory exceptions out of {num_total_excepts_2} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at NMFRecommender exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 exceptions for NMF\n"
     ]
    }
   ],
   "source": [
    "# look at NMFRecommender exceptions\n",
    "nmf_r_excepts = df_tmp.loc[(df_tmp[\"alg_name\"] == \"NMFRecommender\"), \"exception\"].unique()\n",
    "nmf_excepts_small = [x for x in nmf_r_excepts if \"IndexError: index\" not in str(x)]\n",
    "print(f\"{len(nmf_r_excepts)} exceptions for NMF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len([x for x in nmf_r_excepts if (\"memory\" in str(x)) or (\"Memory\" in str(x))]))\n",
    "# print(sum(\n",
    "#     (df_tmp[\"alg_name\"] == \"NMFRecommender\")\n",
    "#     & (df_tmp[\"exception\"].str.contains(\"memory\") | df_tmp[\"exception\"].str.contains(\"Memory\"))\n",
    "# ))\n",
    "# 3\n",
    "# 19\n",
    "# print(len([x for x in nmf_r_excepts if (\"kullback-leibler\" in str(x))]))\n",
    "# print(sum(\n",
    "#     (df_tmp[\"alg_name\"] == \"NMFRecommender\")\n",
    "#     & (df_tmp[\"exception\"].str.contains(\"kullback-leibler\"))\n",
    "# ))\n",
    "# 1\n",
    "# 1462\n",
    "# print(len([x for x in nmf_r_excepts if (\"Input contains NaN,\" in str(x))]))\n",
    "# print(sum(\n",
    "#     (df_tmp[\"alg_name\"] == \"NMFRecommender\")\n",
    "#     & (df_tmp[\"exception\"].str.contains(\"Input contains NaN,\"))\n",
    "# ))\n",
    "# 1\n",
    "# 157\n",
    "# print(len([x for x in nmf_r_excepts if (\"IndexError: index\" in str(x))]))\n",
    "# print(sum(\n",
    "#     (df_tmp[\"alg_name\"] == \"NMFRecommender\")\n",
    "#     & (df_tmp[\"exception\"].str.contains(\"IndexError: index\"))\n",
    "# ))\n",
    "# 31\n",
    "# 782\n",
    "# print(len([x for x in nmf_r_excepts if (\"Negative values in data passed to \" in str(x))]))\n",
    "# print(sum(\n",
    "#     (df_tmp[\"alg_name\"] == \"NMFRecommender\")\n",
    "#     & (df_tmp[\"exception\"].str.contains(\"Negative values in data passed to \"))\n",
    "# ))\n",
    "# 1\n",
    "# 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION**: \n",
    "- So -- there are a lot of errors in NMF...\n",
    "- only a small number of memory errors (19 total!)\n",
    "- a ton of bad parameter sets -- using k-l parameter when it's not allowed (1462 instances)\n",
    "- a bunch of ValueError occurrences (157)\n",
    "- a bunch of IndexError occurrences (782)\n",
    "- NMF can't handle negative values, so 200 occurrences here.\n",
    "\n",
    "I'd just as well ignore all of these issues for now, since some wi?ll take time to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now ItemKNNCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 exceptions for itemknn\n"
     ]
    }
   ],
   "source": [
    "# look at itemknn exceptions\n",
    "itemknn_excepts = df_tmp.loc[(df_tmp[\"alg_name\"] == \"ItemKNNCF\"), \"exception\"].unique()\n",
    "print(f\"{len(itemknn_excepts)} exceptions for itemknn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemKNN: there are 57 memory exceptions out of 87 total\n"
     ]
    }
   ],
   "source": [
    "# how many memory errors?\n",
    "num_mem_excepts_knn = sum(\n",
    "    (df_tmp[\"alg_name\"] == \"ItemKNNCF\")\n",
    "    & (df_tmp[\"exception\"].str.contains(\"memory\") | df_tmp[\"exception\"].str.contains(\"Memory\"))\n",
    ")\n",
    "num_total_excepts_knn = sum(\n",
    "    (df_tmp[\"alg_name\"] == \"ItemKNNCF\")\n",
    "    & ~df_tmp[\"exception\"].isna()\n",
    ")\n",
    "print(f\"itemKNN: there are {num_mem_excepts_knn} memory exceptions out of {num_total_excepts_knn} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonmem_itemknn_excepts = [e for e in itemknn_excepts if (\"memory\" not in str(e)) and (\"Memory\" not in str(e))]\n",
    "\n",
    "# all of these are negative value errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION**:\n",
    "- all of the ItemKNN errors are memory errors... only 57 of these occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now P3alphaRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 exceptions for P3alphaRecommender\n"
     ]
    }
   ],
   "source": [
    "# look at P3alphaRecommender exceptions\n",
    "p3alpha = df_tmp.loc[(df_tmp[\"alg_name\"] == \"P3alphaRecommender\"), \"exception\"].unique()\n",
    "print(f\"{len(p3alpha)} exceptions for P3alphaRecommender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traceback (most recent call last):\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 402, in _objective_function\\n    result_dict, result_string, recommender_instance, train_time, evaluation_time = self._evaluate_on_validation(current_fit_parameters_dict)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/RandomSearch.py\", line 50, in _evaluate_on_validation\\n    current_fit_parameters\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 293, in _evaluate_on_validation\\n    recommender_instance, train_time = self._fit_model(current_fit_parameters)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 283, in _fit_model\\n    **current_fit_parameters)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/GraphBased/P3alphaRecommender.py\", line 138, in fit\\n    self.W_sparse = normalize(self.W_sparse, norm=\\'l1\\', axis=1)\\n  File \"/home/shared/miniconda3/envs/reczilla/lib/python3.6/site-packages/sklearn/preprocessing/data.py\", line 1574, in normalize\\n    estimator=\\'the normalize function\\', dtype=FLOAT_DTYPES)\\n  File \"/home/shared/miniconda3/envs/reczilla/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 517, in check_array\\n    accept_large_sparse=accept_large_sparse)\\n  File \"/home/shared/miniconda3/envs/reczilla/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 350, in _ensure_sparse_format\\n    allow_nan=force_all_finite == \\'allow-nan\\')\\n  File \"/home/shared/miniconda3/envs/reczilla/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 56, in _assert_all_finite\\n    raise ValueError(msg_err.format(type_err, X.dtype))\\nValueError: Input contains NaN, infinity or a value too large for dtype(\\'float32\\').\\n'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3alpha[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION**:\n",
    "- ~100 ValueError instances occur for p3alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now RP3betaRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 exceptions for RP3betaRecommender\n"
     ]
    }
   ],
   "source": [
    "# look at rp3beta exceptions\n",
    "rp3beta = df_tmp.loc[(df_tmp[\"alg_name\"] == \"RP3betaRecommender\"), \"exception\"].unique()\n",
    "print(f\"{len(rp3beta)} exceptions for RP3betaRecommender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traceback (most recent call last):\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 402, in _objective_function\\n    result_dict, result_string, recommender_instance, train_time, evaluation_time = self._evaluate_on_validation(current_fit_parameters_dict)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/RandomSearch.py\", line 50, in _evaluate_on_validation\\n    current_fit_parameters\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 293, in _evaluate_on_validation\\n    recommender_instance, train_time = self._fit_model(current_fit_parameters)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/ParameterTuning/SearchAbstractClass.py\", line 283, in _fit_model\\n    **current_fit_parameters)\\n  File \"/home/shared/reczilla/RecSys2019_DeepLearning_Evaluation/GraphBased/RP3betaRecommender.py\", line 148, in fit\\n    self.W_sparse = normalize(self.W_sparse, norm=\\'l1\\', axis=1)\\n  File \"/home/shared/miniconda3/envs/reczilla/lib/python3.6/site-packages/sklearn/preprocessing/data.py\", line 1574, in normalize\\n    estimator=\\'the normalize function\\', dtype=FLOAT_DTYPES)\\n  File \"/home/shared/miniconda3/envs/reczilla/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 517, in check_array\\n    accept_large_sparse=accept_large_sparse)\\n  File \"/home/shared/miniconda3/envs/reczilla/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 350, in _ensure_sparse_format\\n    allow_nan=force_all_finite == \\'allow-nan\\')\\n  File \"/home/shared/miniconda3/envs/reczilla/lib/python3.6/site-packages/sklearn/utils/validation.py\", line 56, in _assert_all_finite\\n    raise ValueError(msg_err.format(type_err, X.dtype))\\nValueError: Input contains NaN, infinity or a value too large for dtype(\\'float32\\').\\n'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp3beta[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION**:\n",
    "- ~100 ValueError instances occur with rp3beta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Params per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duncan/miniconda3/envs/recsys/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "df_expt_clean = df_expt.loc[df_expt[\"exception\"].isna(), :]\n",
    "\n",
    "knn_rows = df_expt_clean[\"alg_name\"].str.contains(\"KNN\")\n",
    "\n",
    "knn_basename = df_expt_clean.loc[knn_rows, \"alg_name\"].apply(lambda x: x.split(\"_\")[0])\n",
    "knn_sim = df_expt_clean.loc[knn_rows, \"alg_name\"].apply(lambda x: x.split(\"_\")[1])\n",
    "df_expt_clean.loc[knn_rows, \"alg_name\"] = knn_basename  # either UserKNN or ItemKNN\n",
    "\n",
    "\n",
    "num_samples = df_expt_clean.groupby([\"alg_name\", \"dataset_name\"]).size().rename(\"count\").reset_index()\n",
    "\n",
    "# add a col for max samples:\n",
    "num_samples.loc[:, \"max_samples\"] = 100\n",
    "num_samples.loc[num_samples[\"alg_name\"].isin(one_sample_algs), \"max_samples\"] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alg_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>count</th>\n",
       "      <th>max_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>EASE_R_Recommender</td>\n",
       "      <td>AmazonAmazonInstantVideoReader</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonLatinMusicReader</td>\n",
       "      <td>38</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonMP3PlayersAccessoriesReader</td>\n",
       "      <td>38</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonAppstoreforAndroidReader</td>\n",
       "      <td>39</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonBluesReader</td>\n",
       "      <td>39</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonComputersReader</td>\n",
       "      <td>39</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonOfficeSchoolSuppliesReader</td>\n",
       "      <td>39</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonRapHipHopReader</td>\n",
       "      <td>39</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonDavisReader</td>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonFolkReader</td>\n",
       "      <td>41</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonHomeImprovementReader</td>\n",
       "      <td>41</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonRBReader</td>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonHardRockMetalReader</td>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonGospelReader</td>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonCountryReader</td>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonNewAgeReader</td>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonKitchenDiningReader</td>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonWineReader</td>\n",
       "      <td>45</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonRockReader</td>\n",
       "      <td>46</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonSoftwareReader</td>\n",
       "      <td>47</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonJazzReader</td>\n",
       "      <td>48</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonAllElectronicsReader</td>\n",
       "      <td>49</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonPopReader</td>\n",
       "      <td>49</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonAlternativeRockReader</td>\n",
       "      <td>49</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>RP3betaRecommender</td>\n",
       "      <td>AnimeReader</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>RP3betaRecommender</td>\n",
       "      <td>Jester2Reader</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonIndustrialScientificReader</td>\n",
       "      <td>51</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>P3alphaRecommender</td>\n",
       "      <td>Jester2Reader</td>\n",
       "      <td>51</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>P3alphaRecommender</td>\n",
       "      <td>AnimeReader</td>\n",
       "      <td>51</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonAllBeautyReader</td>\n",
       "      <td>52</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>PureSVDRecommender</td>\n",
       "      <td>AmazonCountryReader</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>GowallaReader</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonBeautyReader</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>NetflixPrizeReader</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>SLIM_BPR_Cython</td>\n",
       "      <td>AmazonAppsforAndroidReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>RecipesReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>YahooMusicReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>SLIM_BPR_Cython</td>\n",
       "      <td>AmazonClothingShoesJewelryReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CoClustering</td>\n",
       "      <td>AmazonBooksReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonToolsHomeImprovementReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>MatrixFactorization_AsySVD_Cython</td>\n",
       "      <td>AmazonBooksReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>MatrixFactorization_AsySVD_Cython</td>\n",
       "      <td>AmazonSportsOutdoorsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>MatrixFactorization_AsySVD_Cython</td>\n",
       "      <td>GoogleLocalReviewsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>AmazonCDsVinylReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>AmazonClothingShoesJewelryReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>AmazonElectronicsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>AmazonKindleStoreReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>AmazonSportsOutdoorsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>BookCrossingReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>GowallaReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>MatrixFactorization_FunkSVD_Cython</td>\n",
       "      <td>GowallaReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonAppsforAndroidReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonAutomotiveReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonClothingShoesJewelryReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonElectronicsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonGroceryGourmetFoodReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>SLIM_BPR_Cython</td>\n",
       "      <td>AmazonElectronicsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>NMFRecommender</td>\n",
       "      <td>AmazonOfficeProductsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>MatrixFactorization_FunkSVD_Cython</td>\n",
       "      <td>AmazonSportsOutdoorsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>SLIM_BPR_Cython</td>\n",
       "      <td>AmazonGroceryGourmetFoodReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                alg_name                       dataset_name  \\\n",
       "91                    EASE_R_Recommender     AmazonAmazonInstantVideoReader   \n",
       "702                       NMFRecommender             AmazonLatinMusicReader   \n",
       "704                       NMFRecommender  AmazonMP3PlayersAccessoriesReader   \n",
       "668                       NMFRecommender     AmazonAppstoreforAndroidReader   \n",
       "674                       NMFRecommender                  AmazonBluesReader   \n",
       "683                       NMFRecommender              AmazonComputersReader   \n",
       "711                       NMFRecommender   AmazonOfficeSchoolSuppliesReader   \n",
       "716                       NMFRecommender              AmazonRapHipHopReader   \n",
       "686                       NMFRecommender                  AmazonDavisReader   \n",
       "689                       NMFRecommender                   AmazonFolkReader   \n",
       "695                       NMFRecommender        AmazonHomeImprovementReader   \n",
       "715                       NMFRecommender                     AmazonRBReader   \n",
       "693                       NMFRecommender          AmazonHardRockMetalReader   \n",
       "691                       NMFRecommender                 AmazonGospelReader   \n",
       "684                       NMFRecommender                AmazonCountryReader   \n",
       "709                       NMFRecommender                 AmazonNewAgeReader   \n",
       "701                       NMFRecommender          AmazonKitchenDiningReader   \n",
       "723                       NMFRecommender                   AmazonWineReader   \n",
       "717                       NMFRecommender                   AmazonRockReader   \n",
       "718                       NMFRecommender               AmazonSoftwareReader   \n",
       "699                       NMFRecommender                   AmazonJazzReader   \n",
       "662                       NMFRecommender         AmazonAllElectronicsReader   \n",
       "714                       NMFRecommender                    AmazonPopReader   \n",
       "663                       NMFRecommender        AmazonAlternativeRockReader   \n",
       "982                   RP3betaRecommender                        AnimeReader   \n",
       "991                   RP3betaRecommender                      Jester2Reader   \n",
       "697                       NMFRecommender   AmazonIndustrialScientificReader   \n",
       "817                   P3alphaRecommender                      Jester2Reader   \n",
       "808                   P3alphaRecommender                        AnimeReader   \n",
       "661                       NMFRecommender              AmazonAllBeautyReader   \n",
       "...                                  ...                                ...   \n",
       "855                   PureSVDRecommender                AmazonCountryReader   \n",
       "731                       NMFRecommender                      GowallaReader   \n",
       "673                       NMFRecommender                 AmazonBeautyReader   \n",
       "741                       NMFRecommender                 NetflixPrizeReader   \n",
       "1185                     SLIM_BPR_Cython         AmazonAppsforAndroidReader   \n",
       "742                       NMFRecommender                      RecipesReader   \n",
       "745                       NMFRecommender                   YahooMusicReader   \n",
       "1199                     SLIM_BPR_Cython   AmazonClothingShoesJewelryReader   \n",
       "14                          CoClustering                  AmazonBooksReader   \n",
       "720                       NMFRecommender   AmazonToolsHomeImprovementReader   \n",
       "415    MatrixFactorization_AsySVD_Cython                  AmazonBooksReader   \n",
       "459    MatrixFactorization_AsySVD_Cython         AmazonSportsOutdoorsReader   \n",
       "471    MatrixFactorization_AsySVD_Cython           GoogleLocalReviewsReader   \n",
       "504       MatrixFactorization_BPR_Cython               AmazonCDsVinylReader   \n",
       "508       MatrixFactorization_BPR_Cython   AmazonClothingShoesJewelryReader   \n",
       "514       MatrixFactorization_BPR_Cython            AmazonElectronicsReader   \n",
       "526       MatrixFactorization_BPR_Cython            AmazonKindleStoreReader   \n",
       "545       MatrixFactorization_BPR_Cython         AmazonSportsOutdoorsReader   \n",
       "724                       NMFRecommender                 BookCrossingReader   \n",
       "558       MatrixFactorization_BPR_Cython                      GowallaReader   \n",
       "645   MatrixFactorization_FunkSVD_Cython                      GowallaReader   \n",
       "667                       NMFRecommender         AmazonAppsforAndroidReader   \n",
       "670                       NMFRecommender             AmazonAutomotiveReader   \n",
       "681                       NMFRecommender   AmazonClothingShoesJewelryReader   \n",
       "688                       NMFRecommender            AmazonElectronicsReader   \n",
       "692                       NMFRecommender     AmazonGroceryGourmetFoodReader   \n",
       "1206                     SLIM_BPR_Cython            AmazonElectronicsReader   \n",
       "710                       NMFRecommender         AmazonOfficeProductsReader   \n",
       "632   MatrixFactorization_FunkSVD_Cython         AmazonSportsOutdoorsReader   \n",
       "1210                     SLIM_BPR_Cython     AmazonGroceryGourmetFoodReader   \n",
       "\n",
       "      count  max_samples  \n",
       "91        1          100  \n",
       "702      38          100  \n",
       "704      38          100  \n",
       "668      39          100  \n",
       "674      39          100  \n",
       "683      39          100  \n",
       "711      39          100  \n",
       "716      39          100  \n",
       "686      40          100  \n",
       "689      41          100  \n",
       "695      41          100  \n",
       "715      42          100  \n",
       "693      42          100  \n",
       "691      42          100  \n",
       "684      42          100  \n",
       "709      42          100  \n",
       "701      42          100  \n",
       "723      45          100  \n",
       "717      46          100  \n",
       "718      47          100  \n",
       "699      48          100  \n",
       "662      49          100  \n",
       "714      49          100  \n",
       "663      49          100  \n",
       "982      50          100  \n",
       "991      50          100  \n",
       "697      51          100  \n",
       "817      51          100  \n",
       "808      51          100  \n",
       "661      52          100  \n",
       "...     ...          ...  \n",
       "855      98          100  \n",
       "731      98          100  \n",
       "673      98          100  \n",
       "741      98          100  \n",
       "1185     99          100  \n",
       "742      99          100  \n",
       "745      99          100  \n",
       "1199     99          100  \n",
       "14       99          100  \n",
       "720      99          100  \n",
       "415      99          100  \n",
       "459      99          100  \n",
       "471      99          100  \n",
       "504      99          100  \n",
       "508      99          100  \n",
       "514      99          100  \n",
       "526      99          100  \n",
       "545      99          100  \n",
       "724      99          100  \n",
       "558      99          100  \n",
       "645      99          100  \n",
       "667      99          100  \n",
       "670      99          100  \n",
       "681      99          100  \n",
       "688      99          100  \n",
       "692      99          100  \n",
       "1206     99          100  \n",
       "710      99          100  \n",
       "632      99          100  \n",
       "1210     99          100  \n",
       "\n",
       "[106 rows x 4 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which alg-dataset combinations have fewer than 100 samples?\n",
    "num_samples.loc[num_samples[\"count\"] < num_samples[\"max_samples\"], :].sort_values(by=\"count\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>alg_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>count</th>\n",
       "      <th>max_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>982</td>\n",
       "      <td>RP3betaRecommender</td>\n",
       "      <td>AnimeReader</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>991</td>\n",
       "      <td>RP3betaRecommender</td>\n",
       "      <td>Jester2Reader</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>P3alphaRecommender</td>\n",
       "      <td>Jester2Reader</td>\n",
       "      <td>51</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>808</td>\n",
       "      <td>P3alphaRecommender</td>\n",
       "      <td>AnimeReader</td>\n",
       "      <td>51</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>866</td>\n",
       "      <td>PureSVDRecommender</td>\n",
       "      <td>AmazonHomeImprovementReader</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>888</td>\n",
       "      <td>PureSVDRecommender</td>\n",
       "      <td>AmazonRockReader</td>\n",
       "      <td>87</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>870</td>\n",
       "      <td>PureSVDRecommender</td>\n",
       "      <td>AmazonJazzReader</td>\n",
       "      <td>89</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>877</td>\n",
       "      <td>PureSVDRecommender</td>\n",
       "      <td>AmazonMiscellaneousReader</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>502</td>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>AmazonBooksReader</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>885</td>\n",
       "      <td>PureSVDRecommender</td>\n",
       "      <td>AmazonPopReader</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>834</td>\n",
       "      <td>PureSVDRecommender</td>\n",
       "      <td>AmazonAlternativeRockReader</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>70</td>\n",
       "      <td>CoClustering</td>\n",
       "      <td>GoogleLocalReviewsReader</td>\n",
       "      <td>97</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>299</td>\n",
       "      <td>IALSRecommender</td>\n",
       "      <td>Jester2Reader</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>855</td>\n",
       "      <td>PureSVDRecommender</td>\n",
       "      <td>AmazonCountryReader</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1199</td>\n",
       "      <td>SLIM_BPR_Cython</td>\n",
       "      <td>AmazonClothingShoesJewelryReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1185</td>\n",
       "      <td>SLIM_BPR_Cython</td>\n",
       "      <td>AmazonAppsforAndroidReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14</td>\n",
       "      <td>CoClustering</td>\n",
       "      <td>AmazonBooksReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>645</td>\n",
       "      <td>MatrixFactorization_FunkSVD_Cython</td>\n",
       "      <td>GowallaReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>632</td>\n",
       "      <td>MatrixFactorization_FunkSVD_Cython</td>\n",
       "      <td>AmazonSportsOutdoorsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>558</td>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>GowallaReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>545</td>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>AmazonSportsOutdoorsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>526</td>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>AmazonKindleStoreReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>514</td>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>AmazonElectronicsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>508</td>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>AmazonClothingShoesJewelryReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>504</td>\n",
       "      <td>MatrixFactorization_BPR_Cython</td>\n",
       "      <td>AmazonCDsVinylReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>471</td>\n",
       "      <td>MatrixFactorization_AsySVD_Cython</td>\n",
       "      <td>GoogleLocalReviewsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>459</td>\n",
       "      <td>MatrixFactorization_AsySVD_Cython</td>\n",
       "      <td>AmazonSportsOutdoorsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>415</td>\n",
       "      <td>MatrixFactorization_AsySVD_Cython</td>\n",
       "      <td>AmazonBooksReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1206</td>\n",
       "      <td>SLIM_BPR_Cython</td>\n",
       "      <td>AmazonElectronicsReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1210</td>\n",
       "      <td>SLIM_BPR_Cython</td>\n",
       "      <td>AmazonGroceryGourmetFoodReader</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                            alg_name  \\\n",
       "0     982                  RP3betaRecommender   \n",
       "1     991                  RP3betaRecommender   \n",
       "2     817                  P3alphaRecommender   \n",
       "3     808                  P3alphaRecommender   \n",
       "4     866                  PureSVDRecommender   \n",
       "5     888                  PureSVDRecommender   \n",
       "6     870                  PureSVDRecommender   \n",
       "7     877                  PureSVDRecommender   \n",
       "8     502      MatrixFactorization_BPR_Cython   \n",
       "9     885                  PureSVDRecommender   \n",
       "10    834                  PureSVDRecommender   \n",
       "11     70                        CoClustering   \n",
       "12    299                     IALSRecommender   \n",
       "13    855                  PureSVDRecommender   \n",
       "14   1199                     SLIM_BPR_Cython   \n",
       "15   1185                     SLIM_BPR_Cython   \n",
       "16     14                        CoClustering   \n",
       "17    645  MatrixFactorization_FunkSVD_Cython   \n",
       "18    632  MatrixFactorization_FunkSVD_Cython   \n",
       "19    558      MatrixFactorization_BPR_Cython   \n",
       "20    545      MatrixFactorization_BPR_Cython   \n",
       "21    526      MatrixFactorization_BPR_Cython   \n",
       "22    514      MatrixFactorization_BPR_Cython   \n",
       "23    508      MatrixFactorization_BPR_Cython   \n",
       "24    504      MatrixFactorization_BPR_Cython   \n",
       "25    471   MatrixFactorization_AsySVD_Cython   \n",
       "26    459   MatrixFactorization_AsySVD_Cython   \n",
       "27    415   MatrixFactorization_AsySVD_Cython   \n",
       "28   1206                     SLIM_BPR_Cython   \n",
       "29   1210                     SLIM_BPR_Cython   \n",
       "\n",
       "                        dataset_name  count  max_samples  \n",
       "0                        AnimeReader     50          100  \n",
       "1                      Jester2Reader     50          100  \n",
       "2                      Jester2Reader     51          100  \n",
       "3                        AnimeReader     51          100  \n",
       "4        AmazonHomeImprovementReader     85          100  \n",
       "5                   AmazonRockReader     87          100  \n",
       "6                   AmazonJazzReader     89          100  \n",
       "7          AmazonMiscellaneousReader     90          100  \n",
       "8                  AmazonBooksReader     96          100  \n",
       "9                    AmazonPopReader     96          100  \n",
       "10       AmazonAlternativeRockReader     96          100  \n",
       "11          GoogleLocalReviewsReader     97          100  \n",
       "12                     Jester2Reader     98          100  \n",
       "13               AmazonCountryReader     98          100  \n",
       "14  AmazonClothingShoesJewelryReader     99          100  \n",
       "15        AmazonAppsforAndroidReader     99          100  \n",
       "16                 AmazonBooksReader     99          100  \n",
       "17                     GowallaReader     99          100  \n",
       "18        AmazonSportsOutdoorsReader     99          100  \n",
       "19                     GowallaReader     99          100  \n",
       "20        AmazonSportsOutdoorsReader     99          100  \n",
       "21           AmazonKindleStoreReader     99          100  \n",
       "22           AmazonElectronicsReader     99          100  \n",
       "23  AmazonClothingShoesJewelryReader     99          100  \n",
       "24              AmazonCDsVinylReader     99          100  \n",
       "25          GoogleLocalReviewsReader     99          100  \n",
       "26        AmazonSportsOutdoorsReader     99          100  \n",
       "27                 AmazonBooksReader     99          100  \n",
       "28           AmazonElectronicsReader     99          100  \n",
       "29    AmazonGroceryGourmetFoodReader     99          100  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now drop ease-r and NMF.. these are problematic..\n",
    "# which alg-dataset combinations have fewer than 100 samples?\n",
    "num_samples.loc[(num_samples[\"count\"] < num_samples[\"max_samples\"]) & ~num_samples[\"alg_name\"].isin([\"NMFRecommender\", \"EASE_R_Recommender\"]), :].sort_values(by=\"count\", ascending=True).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION**:\n",
    "- if we exclude the two problem algs - NMF and EASE-R - there are 30 cases where we don't get 100 samples. for these, we have at least 50 samples for all alg-dataset combos.  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a49d437ec7d70416a2164b1de0841ecb25c4cf254d34094d737b88836beceb4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('recsys')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
